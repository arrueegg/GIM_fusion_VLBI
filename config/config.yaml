# General configuration
project_name: "GIM_fusion_VLBI"
output_dir: "experiments/"
year: 2023
doy: 12
random_seed: 42

# Data configuration
data:
  mode: "DTEC_Fusion"    # Fusion or GNSS (only GNSS) or DTEC_Fusion
  GNSS_data_path: "/home/space/data/iono/STEC_DB_estDCB/"
  VLBI_data_path: "/scratch2/arrueegg/WP1/VLBIono/Results/"
  VLBI_raw_data_path: "/home/space/data/vlbi/ivsdata/vgosdb/"
  columns_to_load:  #['station', 'stec', 'vtec', 'sm_lat', 'sm_lon', 'satele', 'satazi', 'sod', 'dcbs', 'dcbr'] # List of columns to load or None (no arg) for all columns
  shuffle: True

# Preprocessing
preprocessing:
  elevation: 15.0
  SH_encoding: True
  SH_degree: 16
  split: 'lists'           # random or lists

# Model configuration
model:
  ensemble_size: 5
  model_type: "MLP"
  input_size: 39           # Input feature size
  hidden_size: [256,256,256]    # Hidden layer size
  output_size: 1           # Output prediction size
  activation: "tanh"       # Activation function: relu, tanh, sigmoid, leaky_relu
  apply_softplus: false    # Apply softplus for positive-only outputs
  dropout: 0.0             # Dropout rate for regularization

# Training configuration
training:
  test_size: 0.2          # Proportion of data to use for testing
  epochs: 20
  batchsize: 512
  vlbi_sampling_weight: 1.0   # Weight for VLBI data sampling relative to GNSS data. Set to 1.0 for no weighting. higher weighting means sampled more frequently
  num_workers: 4
  learning_rate: 0.001
  optimizer: "Adam"        # Optimizer: SGD, Adam, AdamW, RMSprop, Adagrad, Adadelta
  weight_decay: 0
  loss_function: "LaplaceLoss"  # Loss function: MSELoss, MAELoss, HuberLoss, LaplaceLoss, GaussianNLLLoss
  weighted_loss: true      # Whether to use weighted loss
  gnss_loss_weight: 1.0    # GNSS loss weight
  vlbi_loss_weight: 1.0    # VLBI loss weight
  scheduler: "cosine_annealing"     # Scheduler: none, step_lr, cosine_annealing
  scheduler_step_size: 10  # Step size for scheduler (reduces lr every X epochs)
  scheduler_gamma: 0.1     # Gamma for scheduler (gamma ist the reduction factor lr_new = lr_old * gamma)
  early_stopping: true     # Whether to use early stopping
  patience: 20             # Patience for early stopping
  overfit_single_batch: False  # Sanity check to test if model is able to overfit on a single batch
  save_model_every_epoch: True  # Whether to save the model every epoch

# Debugging configuration
debugging:
  debug: False           # True = run in debug mode (no wandb logging)
  disable_tqdm: False              # Disable tqdm progress bar (useful on cluster)
  log_level: "DEBUG"               # Logging level (e.g., DEBUG, INFO)
  debug_data_dir: "experiments/debug/"  # Directory for saving debug data
